{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchの基礎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- テンソル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1 = tensor([1, 2, 3, 4]), t1.shape = torch.Size([4])\n",
      "t2.shape = torch.Size([32, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3, 4])\n",
    "t2 = torch.zeros(([32, 3, 128, 128]))\n",
    "\n",
    "print(f't1 = {t1}, t1.shape = {t1.shape}')\n",
    "print(f't2.shape = {t2.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- テンソルのGPUへの転送"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2, 3, 4], device='cuda') # 生成時からGPUメモリ上\n",
    "\n",
    "t2 = torch.tensor([1, 2, 3, 4])\n",
    "t2 = t2.to('cuda') # 生成はメインメモリ、後からGPUメモリに転送"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python演算子を使ったテンソルの演算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 = tensor([ 3,  6,  9, 12])\n",
      "t4 = tensor([ 1,  4,  9, 16])\n"
     ]
    }
   ],
   "source": [
    "# 演算は要素ごとに行われる\n",
    "t1 = torch.tensor([1, 2, 3, 4])\n",
    "t2 = torch.tensor([2, 4, 6, 8])\n",
    "\n",
    "t3 = t1 + t2\n",
    "t4 = t1 ** 2\n",
    "print(f't3 = {t3}')\n",
    "print(f't4 = {t4}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- テンソルを処理する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1.shape = torch.Size([2, 2])\n",
      "t2.shape = torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# テンソルの形を変形するview関数\n",
    "# メモリ上のデータ配置と見かけ上の形で整合性が取れない場合view関数使えない\n",
    "# view関数が使えない場合はreshape関数を使用する\n",
    "# reshape関数は整合性取れない場合はデータを複製して形を変える\n",
    "t1 = torch.tensor([1, 2, 3, 4])\n",
    "t1 = t1.view(2, 2)\n",
    "\n",
    "t2 = torch.tensor([1, 2, 3, 4, 5, 6])\n",
    "t2 = t2.view(2, -1)\n",
    "\n",
    "print(f't1.shape = {t1.shape}')\n",
    "print(f't2.shape = {t2.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- transpose関数とpermute関数による軸の並び替え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1.shape = torch.Size([128, 3, 32, 128])\n",
      "t2.shape = torch.Size([128, 32, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "# transpose関数：任意の2軸を入れ替える\n",
    "# permute関数はtranspose関数の拡張版　すべての軸を一度に並び替えることが可能\n",
    "t1 = torch.zeros((32, 3, 128, 128))\n",
    "t1 = t1.transpose(0, 2)\n",
    "\n",
    "t2 = torch.zeros((32, 3, 128, 128))\n",
    "t2 = t2.permute(2, 0, 3, 1)\n",
    "\n",
    "print(f't1.shape = {t1.shape}')\n",
    "print(f't2.shape = {t2.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cat関数とstack関数による複数テンソルの連結"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 = tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "t6 = tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "# cat関数：テンソルが持つ既存の軸の1つで複数のテンソルを連結\n",
    "# stack関数：新しく軸を追加し、その軸で複数のテンソルを連結\n",
    "t1 = torch.tensor([1, 2, 3, 4, 5, 6]).view(2, 3)\n",
    "t2 = torch.tensor([7, 8, 9]).view(1, 3)\n",
    "t3 = torch.cat((t1, t2))\n",
    "\n",
    "t4 = torch.tensor([1, 2, 3])\n",
    "t5 = torch.tensor([4, 5, 6])\n",
    "t6 = torch.stack((t4, t5), dim=1) # dim 連結する軸　デフォルト0軸（列方向）\n",
    "\n",
    "print(f't3 = {t3}')\n",
    "print(f't6 = {t6}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- インデクシングによるテンソル要素の抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2 = tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "t3 = tensor([[1, 3],\n",
      "        [4, 6],\n",
      "        [7, 9]])\n",
      "t4 = tensor([2, 9, 5])\n",
      "t5 = tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "t6 = tensor([2, 4, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(3, 3)\n",
    "t2 = t1[[0, 1], :]\n",
    "t3 = t1[:, [0, 2]]\n",
    "t4 = t1[[0, 2, 1], [1, 2, 1]] # 0行目1列目、2行目2列目、1行目1列目\n",
    "t5 = t1[[True, True, False]]\n",
    "t6 = t1[t1 % 2 == 0]\n",
    "\n",
    "print(f't2 = {t2}')\n",
    "print(f't3 = {t3}')\n",
    "print(f't4 = {t4}')\n",
    "print(f't5 = {t5}')\n",
    "print(f't6 = {t6}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ブロードキャストを使った演算\n",
    "- ブロードキャストが起きる条件\n",
    "<br>2つのテンソルの軸数が1以上\n",
    "<br>2つのテンソルを最終軸から比較した場合、各軸の次元が同じであるか。どちらかが1であるか、どちらかの軸が存在しない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3 = tensor([[4, 5, 6],\n",
      "        [5, 6, 7]])\n",
      "t6 = tensor([[ 4,  6],\n",
      "        [ 6,  8],\n",
      "        [ 8, 10]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2]).view(2, 1)\n",
    "t2 = torch.tensor([3, 4, 5])\n",
    "\n",
    "t3 = t1 + t2\n",
    "\n",
    "t4 = torch.tensor([1, 2, 3, 4, 5, 6]).view(3, 2)\n",
    "t5 = torch.tensor([3, 4])\n",
    "\n",
    "t6 = t4 + t5\n",
    "\n",
    "print(f't3 = {t3}')\n",
    "print(f't6 = {t6}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PyTorchのモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多クラスロジスティック回帰モデル\n",
    "class MultiClassLogisticRegression(nn.Module):\n",
    "    def __init__(self, dim_input: int, num_class: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(dim_input, num_class)\n",
    "        \n",
    "    \"\"\"\n",
    "    順伝播関数\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        l = self.linear(x)\n",
    "        y = l.softmax(dim=1)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight: shape = torch.Size([10, 3072])\n",
      "linear.bias: shape = torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# 多クラスロジスティック回帰モデルの使用例\n",
    "model = MultiClassLogisticRegression(32 * 32 * 3, 10)\n",
    "\n",
    "# 学習モード\n",
    "model.train()\n",
    "\n",
    "# 推論モード\n",
    "model.eval()\n",
    "\n",
    "x = torch.normal(0, 1, size=(1, 32 * 32 * 3))\n",
    "y = model(x)\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    print(f'{name}: shape = {parameter.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sequentialクラス\n",
    "<br>複数の処理を直列にまとめて適用するためのクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNSequential(nn.Module):\n",
    "    def __init__(self, dim_input: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim_input, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.layers(x)\n",
    "        y = l.softmax(dim=1)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ModuleList\n",
    "<br>複数の処理をリストにまとめて保存しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNNModuleList(nn.Module):\n",
    "    def __init__(self, dim_input: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Linear(dim_input, 256)]\n",
    "        layers += [nn.Linear(256, 256) for _ in range(2)]\n",
    "        layers.append(nn.Linear(256, num_classes))\n",
    "        self.layers = nn.ModuleList(layers) # ModuleListクラスを使う\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        l = self.layers[-1](x)\n",
    "        y = l.softmax(dim=1)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_sequential = tensor([[0.1028, 0.1018, 0.0925, 0.1045, 0.1017, 0.1001, 0.1001, 0.0970, 0.1051,\n",
      "         0.0944]], grad_fn=<SoftmaxBackward0>)\n",
      "y_modulelist = tensor([[0.1074, 0.1023, 0.1003, 0.0939, 0.1049, 0.0999, 0.0962, 0.0981, 0.0926,\n",
      "         0.1043]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_sequential = FNNSequential(32 * 32 * 3, 10)\n",
    "model_modulelist = FNNModuleList(32 * 32 * 3, 10)\n",
    "\n",
    "model_sequential.eval()\n",
    "model_modulelist.eval()\n",
    "\n",
    "x = torch.normal(0, 1, size=(1, 32 * 32 * 3))\n",
    "y_sequential = model_sequential(x)\n",
    "y_modulelist = model_modulelist(x)\n",
    "\n",
    "print(f'y_sequential = {y_sequential}')\n",
    "print(f'y_modulelist = {y_modulelist}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自動微分\n",
    "<br>PyTorchがグラフ構造と順伝播時の個々の計算処理の勾配計算を記録している"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight.grad = tensor([[ 0.9379, -0.4124, -0.3194,  ..., -0.6923,  0.3473,  0.2563],\n",
      "        [-0.0905,  0.0398,  0.0308,  ...,  0.0668, -0.0335, -0.0247],\n",
      "        [-0.1278,  0.0562,  0.0435,  ...,  0.0943, -0.0473, -0.0349],\n",
      "        ...,\n",
      "        [-0.0537,  0.0236,  0.0183,  ...,  0.0397, -0.0199, -0.0147],\n",
      "        [-0.0665,  0.0292,  0.0226,  ...,  0.0491, -0.0246, -0.0182],\n",
      "        [-0.0715,  0.0315,  0.0244,  ...,  0.0528, -0.0265, -0.0196]])\n",
      "linear.bias.grad = tensor([-0.9625,  0.0929,  0.1312,  0.2564,  0.1196,  0.0751,  0.0906,  0.0552,\n",
      "         0.0682,  0.0734])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(32 * 32 * 3, 10)\n",
    "\n",
    "# 入力ベクトル\n",
    "x = torch.normal(0, 1, size=(1, 32 * 32 * 3))\n",
    "y = torch.tensor([0])\n",
    "\n",
    "# 目的関数（交差エントロピー誤差）の計算\n",
    "y_pred = linear(x)\n",
    "loss = F.cross_entropy(y_pred, y)\n",
    "\n",
    "# 誤差逆伝播（自動微分）\n",
    "loss.backward()\n",
    "\n",
    "print(f'linear.weight.grad = {linear.weight.grad}')\n",
    "print(f'linear.bias.grad = {linear.bias.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
