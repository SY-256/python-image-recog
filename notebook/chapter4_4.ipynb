{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第4章画像分類（その4）\n",
    "#### 精度向上のテクニック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from src import utils, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パラメータ初期化の追加\n",
    "\"\"\"\n",
    "パラメータ初期化関数\n",
    "\"\"\"\n",
    "def _reset_parameters(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # Heらが提案した正規分布を使って初期化\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_in\",\n",
    "                                    nonlinearity=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18の実装\n",
    "class ResNet18(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet18モデル\n",
    "    num_classes : 分類対象の物体モデル数\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2,\n",
    "                               padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            model.BasicBlock(64, 64),\n",
    "            model.BasicBlock(64, 64),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            model.BasicBlock(64, 128, stride=2),\n",
    "            model.BasicBlock(128, 128),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            model.BasicBlock(128, 256, stride=2),\n",
    "            model.BasicBlock(256, 256),\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            model.BasicBlock(256, 512, stride=2),\n",
    "            model.BasicBlock(512, 512),\n",
    "        )\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # ドロップアウトの追加\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "    \n",
    "    \"\"\"\n",
    "    パラメータの初期化関数\n",
    "    \"\"\"\n",
    "    def _reset_parameters(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Heらが考案した正規分布を使って初期化\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in',\n",
    "                                        nonlinearity='relu')\n",
    "    \n",
    "    \"\"\"\n",
    "    順伝播関数\n",
    "    x            : 入力, [バッチサイズ, 入力チャネル数, 高さ, 幅]\n",
    "    return_embed : 特徴量を返すかロジットを返すかを選択する真偽値\n",
    "    \"\"\"\n",
    "    def forward(self, x: torch.Tensor, return_embed: bool=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avg_pool(x)\n",
    "        x = x.flatten(1)\n",
    "        \n",
    "        if return_embed:\n",
    "            return x\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \"\"\"\n",
    "    モデルパラメータが保存されているデバイスを返す関数\n",
    "    \"\"\"\n",
    "    def get_device(self):\n",
    "        return self.linear.weight.device\n",
    "    \n",
    "    \"\"\"\n",
    "    モデルを複製して返す関数\n",
    "    \"\"\"\n",
    "    def copy(self):\n",
    "        return copy.deepcopy(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    '''\n",
    "    ハイパーパラメータとオプションの設定\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.val_ratio = 0.2   # 検証に使う学習セット内のデータの割合\n",
    "        self.num_epochs = 30   # 学習エポック数\n",
    "        self.lr_drop = 25      # 学習率を減衰させるエポック\n",
    "        self.lr = 1e-2         # 学習率\n",
    "        self.moving_avg = 20   # 移動平均で計算する損失と正確度の値の数\n",
    "        self.batch_size = 32   # バッチサイズ\n",
    "        self.num_workers = 2   # データローダに使うCPUプロセスの数\n",
    "        self.device = 'cuda'   # 学習に使うデバイス\n",
    "        self.num_samples = 200 # t-SNEでプロットするサンプル数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・評価を行う関数\n",
    "def train_eval():\n",
    "    config = Config()\n",
    "    \n",
    "    # 入力データ正規化のために学習セットのデータを使って\n",
    "    # 各チャネルの平均と標準偏差を計算\n",
    "    dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"../chapter3-data/\", train=True, download=True,\n",
    "        transform=T.ToTensor()\n",
    "    )\n",
    "    channel_mean, channel_std = utils.get_dataset_statistics(dataset)\n",
    "    \n",
    "    # 画像の整形を行うクラスのインスタンスを用意\n",
    "    train_transforms = T.Compose((\n",
    "        T.RandomResizedCrop(32, scale=(0.8, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=channel_mean, std=channel_std),\n",
    "    ))\n",
    "    test_transforms = T.Compose((\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=channel_mean, std=channel_std),\n",
    "    ))\n",
    "    \n",
    "    # 学習、評価セットの用意\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"../chapter3-data/\", train=True, download=True,\n",
    "        transform=train_transforms,\n",
    "    )\n",
    "    val_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"../chapter3-data/\", train=True, download=True,\n",
    "        transform=test_transforms,\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"../chapter3-data/\", train=False, download=True,\n",
    "        transform=test_transforms,\n",
    "    )\n",
    "    \n",
    "    # 学習・検証セットへ分割するためのインデックス集合の生成\n",
    "    val_set, train_set = utils.generate_subset(\n",
    "        train_dataset, config.val_ratio\n",
    "    )\n",
    "    \n",
    "    print(f'学習セットのサンプル数: {len(train_set)}')\n",
    "    print(f'検証セットのサンプル数: {len(val_set)}')\n",
    "    print(f'テストセットのサンプル数: {len(test_dataset)}')\n",
    "    \n",
    "    # インデックス集合から無作為にインデックスをサンプルするサンプラー\n",
    "    train_sampler = SubsetRandomSampler(train_set)\n",
    "    \n",
    "    # DataLoaderを生成\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers, sampler=train_sampler\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers, sampler=val_set\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers\n",
    "    )\n",
    "    \n",
    "    # 目的関数の生成\n",
    "    loss_func = F.cross_entropy\n",
    "    \n",
    "    # 検証セットの結果による最良モデルの保存変数\n",
    "    val_loss_best = float('inf')\n",
    "    model_best = None\n",
    "    \n",
    "    # ResNet18モデルの生成\n",
    "    model = ResNet18(len(train_dataset.classes))\n",
    "    \n",
    "    # モデルを指定デバイスに転送\n",
    "    model.to(config.device)\n",
    "    \n",
    "    # 最適化器の生成\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.lr,\n",
    "                          momentum=0.9, weight_decay=1e-5)\n",
    "    \n",
    "    # 学習率減衰を管理するスケジューラの生成\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[config.lr_drop], gamma=0.1\n",
    "    )\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        with tqdm(train_loader) as pbar:\n",
    "            pbar.set_description(f'[エポック {epoch+1}]')\n",
    "            \n",
    "            # 移動平均計算用\n",
    "            losses = deque()\n",
    "            accs = deque()\n",
    "            for x, y in pbar:\n",
    "                x = x.to(model.get_device())\n",
    "                y = y.to(model.get_device())\n",
    "                \n",
    "                # パラメータの勾配をリセット\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 順伝播\n",
    "                y_pred = model(x)\n",
    "                \n",
    "                # 学習データに対する損失と正確度を計算\n",
    "                loss = loss_func(y_pred, y)\n",
    "                accuracy = (y_pred.argmax(dim=1) == y).float().mean()\n",
    "                \n",
    "                # 誤差逆伝播\n",
    "                loss.backward()\n",
    "                \n",
    "                # パラメータの更新\n",
    "                optimizer.step()\n",
    "                \n",
    "                # 移動平均を計算して表示\n",
    "                losses.append(loss.item())\n",
    "                accs.append(accuracy.item())\n",
    "                if len(losses) > config.moving_avg:\n",
    "                    losses.popleft()\n",
    "                    accs.popleft()\n",
    "                pbar.set_postfix({\n",
    "                    'loss': torch.Tensor(losses).mean().item(),\n",
    "                    'accuracy': torch.Tensor(accs).mean().item()\n",
    "                })\n",
    "                \n",
    "        \n",
    "        # 検証セットを使って精度評価\n",
    "        val_loss, val_accuracy = utils.evaluate(\n",
    "            val_loader, model, loss_func\n",
    "        )\n",
    "        print(f'検証: loss = {val_loss:.3f}, accuracy = {val_accuracy:.3f}')\n",
    "        \n",
    "        # より良い検証結果が得られた場合、モデルを記録\n",
    "        if val_loss < val_loss_best:\n",
    "            val_loss_best = val_loss\n",
    "            model_best = model.copy()\n",
    "            \n",
    "        # エポック終了時にスケジューラ更新\n",
    "        scheduler.step()\n",
    "        \n",
    "    # テスト\n",
    "    test_loss, test_accuracy = utils.evaluate(\n",
    "        test_loader, model_best, loss_func\n",
    "    )\n",
    "    print(f'テスト: loss = {test_loss:.3f}, accuracy = {test_accuracy:.3f}')\n",
    "    \n",
    "    # t-SNEを使用して特徴量の分布をプロット\n",
    "    utils.plot_t_sne(test_loader, model_best, config.num_samples)\n",
    "    \n",
    "    # モデルパラメータを保存\n",
    "    torch.save(model_best.state_dict(), 'ResNet18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "学習セットのサンプル数: 40000\n",
      "検証セットのサンプル数: 10000\n",
      "テストセットのサンプル数: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[エポック 1]:   0%|          | 0/1250 [00:00<?, ?it/s, loss=2.69, accuracy=0.0625]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/python-image-recog/notebook/chapter4_4.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_eval()\n",
      "\u001b[1;32m/workspaces/python-image-recog/notebook/chapter4_4.ipynb Cell 7\u001b[0m in \u001b[0;36mtrain_eval\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m \u001b[39m# 順伝播\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m    <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39m# 学習データに対する損失と正確度を計算\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(y_pred, y)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/workspaces/python-image-recog/notebook/chapter4_4.ipynb Cell 7\u001b[0m in \u001b[0;36mResNet18.forward\u001b[0;34m(self, x, return_embed)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mavg_pool(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=69'>70</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mflatten(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B633a5c507974686f6e5c4b6167676c655c53747564795c707974686f6e2d696d6167652d7265636f67/workspaces/python-image-recog/notebook/chapter4_4.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mif\u001b[39;00m return_embed:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1130\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1131\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/pooling.py:1179\u001b[0m, in \u001b[0;36mAdaptiveAvgPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49madaptive_avg_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1213\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[0;34m(input, output_size)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n\u001b[1;32m   1212\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[39minput\u001b[39m,), \u001b[39minput\u001b[39m, output_size)\n\u001b[0;32m-> 1213\u001b[0m _output_size \u001b[39m=\u001b[39m _list_with_default(output_size, \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msize())\n\u001b[1;32m   1214\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39madaptive_avg_pool2d(\u001b[39minput\u001b[39m, _output_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
